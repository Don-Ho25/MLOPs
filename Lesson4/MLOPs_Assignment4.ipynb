{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e94838ff-0f92-4a7e-9485-b11ef3339d09",
   "metadata": {},
   "source": [
    "## Assignment 4: Applying AutoML with Ludwig on MNIST Dataset\n",
    "  \n",
    "**DONG**\n",
    "\n",
    "### Part 1: Conceptual Questions\n",
    "\n",
    "##### 1. Describing AutoML\n",
    "\n",
    "Automated Machine Learning (AutoML) is the process of automating the end-to-end steps of applying machine learning to real-world problems, encompassing tasks like feature engineering, model selection, hyperparameter optimization, and iterative evaluation. Its primary benefits include significantly reducing the time and expertise required to achieve a baseline high-performing model, democratizing ML access, and accelerating experimentation. However, limitations include the computational cost of searching large model spaces, reduced transparency (less control over the final pipeline), and the inability of current AutoML tools to fully automate creative problem framing or expert domain knowledge application.\n",
    "\n",
    "\n",
    "\n",
    "##### 2. Explaining Ludwig's Role\n",
    "\n",
    "Ludwig simplifies the creation of machine learning models by providing a `declarative, code-free approach` where users define the task using a simple YAML configuration file, specifying the input data columns and the target output columns. Ludwig then handles the underlying TensorFlow/PyTorch model architecture creation, training, and hyperparameter management automatically. It is designed to address a wide range of supervised learning problems, including regression, classification, sequence tagging, machine translation, and multimodal tasks involving combinations of text, images, and numerical data.\n",
    "\n",
    "\n",
    "\n",
    "##### 3. Discussing Data Preparation in AutoML\n",
    "\n",
    "Data preparation is critically important in an AutoML workflow because the quality and structure of the input data directly constrain the performance of any automatically optimized model. Ludwig handles data preprocessing automatically by mapping data types specified in the configuration (e.g., text, image, numerical) to the appropriate encoders and preprocessors (e.g., tokenizers for text, normalization for numbers). This automatic preprocessing, which is defined implicitly by the input and output types chosen by the user, eliminates the manual coding typically required for feature engineering and preparation, ensuring data is transformed into a format consumable by neural network models.\n",
    "\n",
    "\n",
    "\n",
    "##### 4. Describing Model Serving in AutoML\n",
    "\n",
    "Model serving is the process of deploying a trained ML model into a production environment where it can receive live data inputs (inference requests) and return predictions with low latency. Ludwig supports model serving by exporting trained models in a standardized format that can be easily integrated into common serving infrastructures. Specifically, Ludwig models can be exported into the standard SavedModel format for TensorFlow, making them readily deployable using high-performance serving frameworks like TensorFlow Serving, thereby allowing the model to be exposed via a low-latency REST or gRPC API endpoint.\n",
    "\n",
    "\n",
    "\n",
    "##### 5. Predicting the Future of Open-Source AutoML Tools\n",
    "\n",
    "The future of open-source AutoML tools like Ludwig is bright, driven by increasing community contributions and transparency. Compared to proprietary tools, open-source platforms offer superior accessibility (free to use and inspect the code) and \n",
    "community support (rapid bug fixes, diverse integration possibilities, and extensive documentation). While proprietary tools may currently lead in certain advanced, proprietary algorithms or offer more tightly integrated cloud infrastructure, open-source tools will continue to close the functionality gap, focusing on better interoperability, multimodal support, and automated hyperparameter optimization, ultimately becoming the standard choice for researchers and small-to-midsize enterprises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9c1702-bb68-4ccb-8aed-2dba78938dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "MNIST Handwritten Digit Classification with Ludwig\n",
    "===================================================\n",
    "\n",
    "This script demonstrates how to use Ludwig to build, train, and evaluate\n",
    "a deep learning model for the MNIST dataset.\n",
    "\n",
    "Setup Instructions:\n",
    "------------------\n",
    "1. Create a Python 3.8+ virtual environment (3.6 also works but is deprecated):\n",
    "   python3 -m venv ludwig_env\n",
    "   source ludwig_env/bin/activate  # On Windows: ludwig_env\\Scripts\\activate\n",
    "\n",
    "2. Install Ludwig with image support:\n",
    "   pip install ludwig[full]\n",
    "   # Or minimal: pip install ludwig\n",
    "\n",
    "3. Download the MNIST CSV files to your working directory\n",
    "\n",
    "Usage:\n",
    "------\n",
    "python mnist_ludwig.py\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b032467-4441-4515-a6db-3ea46c979fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ludwig\n",
      "  Using cached ludwig-0.10.4.tar.gz (1.1 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting Cython>=0.25 (from ludwig)\n",
      "  Using cached cython-3.2.2-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: h5py!=3.0.0,>=2.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from ludwig) (3.12.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from ludwig) (2.1.3)\n",
      "Collecting pandas!=1.1.5,<2.2.0,>=1.0 (from ludwig)\n",
      "  Using cached pandas-2.1.4.tar.gz (4.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  pip subprocess to install build dependencies did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [57 lines of output]\n",
      "  Ignoring oldest-supported-numpy: markers 'python_version < \"3.12\"' don't match your environment\n",
      "  Collecting meson-python==0.13.1\n",
      "    Using cached meson_python-0.13.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "  Collecting meson==1.2.1\n",
      "    Using cached meson-1.2.1-py3-none-any.whl.metadata (1.7 kB)\n",
      "  Collecting wheel\n",
      "    Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "  Collecting Cython<3,>=0.29.33\n",
      "    Using cached Cython-0.29.37-py2.py3-none-any.whl.metadata (3.1 kB)\n",
      "  Collecting numpy<2,>=1.26.0\n",
      "    Using cached numpy-1.26.4.tar.gz (15.8 MB)\n",
      "    Installing build dependencies: started\n",
      "    Installing build dependencies: finished with status 'done'\n",
      "    Getting requirements to build wheel: started\n",
      "    Getting requirements to build wheel: finished with status 'done'\n",
      "    Installing backend dependencies: started\n",
      "    Installing backend dependencies: finished with status 'done'\n",
      "    Preparing metadata (pyproject.toml): started\n",
      "    Preparing metadata (pyproject.toml): finished with status 'error'\n",
      "    error: subprocess-exited-with-error\n",
      "  \n",
      "    Preparing metadata (pyproject.toml) did not run successfully.\n",
      "    exit code: 1\n",
      "  \n",
      "    [21 lines of output]\n",
      "    + C:\\Users\\LENOVO\\anaconda3\\python.exe C:\\Users\\LENOVO\\AppData\\Local\\Temp\\pip-install-kzft3pu2\\numpy_571a320205f34ebd901b2c75ba7b6fc8\\vendored-meson\\meson\\meson.py setup C:\\Users\\LENOVO\\AppData\\Local\\Temp\\pip-install-kzft3pu2\\numpy_571a320205f34ebd901b2c75ba7b6fc8 C:\\Users\\LENOVO\\AppData\\Local\\Temp\\pip-install-kzft3pu2\\numpy_571a320205f34ebd901b2c75ba7b6fc8\\.mesonpy-_bu0ouq8 -Dbuildtype=release -Db_ndebug=if-release -Db_vscrt=md --native-file=C:\\Users\\LENOVO\\AppData\\Local\\Temp\\pip-install-kzft3pu2\\numpy_571a320205f34ebd901b2c75ba7b6fc8\\.mesonpy-_bu0ouq8\\meson-python-native-file.ini\n",
      "    The Meson build system\n",
      "    Version: 1.2.99\n",
      "    Source dir: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\pip-install-kzft3pu2\\numpy_571a320205f34ebd901b2c75ba7b6fc8\n",
      "    Build dir: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\pip-install-kzft3pu2\\numpy_571a320205f34ebd901b2c75ba7b6fc8\\.mesonpy-_bu0ouq8\n",
      "    Build type: native build\n",
      "    Project name: NumPy\n",
      "    Project version: 1.26.4\n",
      "    WARNING: Failed to activate VS environment: Could not find C:\\Program Files (x86)\\Microsoft Visual Studio\\Installer\\vswhere.exe\n",
      "  \n",
      "    ..\\meson.build:1:0: ERROR: Unknown compiler(s): [['icl'], ['cl'], ['cc'], ['gcc'], ['clang'], ['clang-cl'], ['pgcc']]\n",
      "    The following exception(s) were encountered:\n",
      "    Running `icl \"\"` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "    Running `cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "    Running `cc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "    Running `gcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "    Running `clang --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "    Running `clang-cl /?` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "    Running `pgcc --version` gave \"[WinError 2] The system cannot find the file specified\"\n",
      "  \n",
      "    A full log can be found at C:\\Users\\LENOVO\\AppData\\Local\\Temp\\pip-install-kzft3pu2\\numpy_571a320205f34ebd901b2c75ba7b6fc8\\.mesonpy-_bu0ouq8\\meson-logs\\meson-log.txt\n",
      "    [end of output]\n",
      "  \n",
      "    note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  error: metadata-generation-failed\n",
      "  \n",
      "  Encountered error while generating package metadata.\n",
      "  \n",
      "  See above for output.\n",
      "  \n",
      "  note: This is an issue with the package mentioned above, not pip.\n",
      "  hint: See above for details.\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: subprocess-exited-with-error\n",
      "\n",
      "pip subprocess to install build dependencies did not run successfully.\n",
      "exit code: 1\n",
      "\n",
      "See above for output.\n",
      "\n",
      "note: This error originates from a subprocess, and is likely not a problem with pip.\n"
     ]
    }
   ],
   "source": [
    "# !python3 -m venv ludwig_env\n",
    "# !ludwig_env\\Scripts\\activate\n",
    "!pip install ludwig\n",
    "# !pip install torch torchvision torchaudio\n",
    "# !pip install jupyterlab-server jupyter-events --upgrade\n",
    "# !ludwig train --config_file config.yaml --dataset data.csv\n",
    "# !pip uninstall jsonschema\n",
    "# !pip install jsonschema==4.6.2\n",
    "\n",
    "# !pip install fastapi uvicorn httpx\n",
    "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d4826a8-2b01-4e4c-bc52-e85225e42328",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'kagglehub'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkagglehub\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Download latest version\u001b[39;00m\n\u001b[0;32m      4\u001b[0m path \u001b[38;5;241m=\u001b[39m kagglehub\u001b[38;5;241m.\u001b[39mdataset_download(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhojjatk/mnist-dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'kagglehub'"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"hojjatk/mnist-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fcfa69a2-92a7-4d99-b7e8-0cd68feac06b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ludwig'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mludwig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LudwigModel\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ludwig'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "from ludwig.api import LudwigModel\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad42a087-ef4c-416d-afad-ab79e42eafb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MNIST Classification with Ludwig\n",
      "============================================================\n",
      "\n",
      "Step 1: Downloading data...\n",
      "Downloading mnist_data\\mnist_train.csv...\n",
      "Downloaded mnist_data\\mnist_train.csv\n",
      "Downloading mnist_data\\mnist_test.csv...\n",
      "Downloaded mnist_data\\mnist_test.csv\n",
      "\n",
      "Step 2: Preparing data...\n",
      "\n",
      "Loading data...\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 279\u001b[0m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  model = LudwingModel.load(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 279\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[23], line 241\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Step 2: Prepare data\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep 2: Preparing data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m train_file, test_file \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Step 3: Choose configuration\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStep 3: Creating model configuration...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 30\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLoading data...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Load CSV files\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# First column is the label, remaining 784 columns are pixel values (28x28)\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTRAIN_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m test_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(TEST_FILE, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Rename columns for clarity\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1925\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2061\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 5, saw 2\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "DATA_DIR = \"mnist_data\"\n",
    "TRAIN_URL = \"https://pjreddie.com/media/files/mnist_train.csv\"\n",
    "TEST_URL = \"https://pjreddie.com/media/files/mnist_test.csv\"\n",
    "TRAIN_FILE = os.path.join(DATA_DIR, \"mnist_train.csv\")\n",
    "TEST_FILE = os.path.join(DATA_DIR, \"mnist_test.csv\")\n",
    "\n",
    "\n",
    "def download_data():\n",
    "    \"\"\"Download MNIST CSV files if not already present.\"\"\"\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    \n",
    "    for url, filepath in [(TRAIN_URL, TRAIN_FILE), (TEST_URL, TEST_FILE)]:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Downloading {filepath}...\")\n",
    "            response = requests.get(url)\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            print(f\"Downloaded {filepath}\")\n",
    "        else:\n",
    "            print(f\"{filepath} already exists\")\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    \"\"\"Load and prepare the MNIST data.\"\"\"\n",
    "    print(\"\\nLoading data...\")\n",
    "    \n",
    "    # Load CSV files\n",
    "    # First column is the label, remaining 784 columns are pixel values (28x28)\n",
    "    train_df = pd.read_csv(TRAIN_FILE, header=None)\n",
    "    test_df = pd.read_csv(TEST_FILE, header=None)\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    train_df.columns = ['label'] + [f'pixel_{i}' for i in range(784)]\n",
    "    test_df.columns = ['label'] + [f'pixel_{i}' for i in range(784)]\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    train_processed = os.path.join(DATA_DIR, \"mnist_train_processed.csv\")\n",
    "    test_processed = os.path.join(DATA_DIR, \"mnist_test_processed.csv\")\n",
    "    \n",
    "    train_df.to_csv(train_processed, index=False)\n",
    "    test_df.to_csv(test_processed, index=False)\n",
    "    \n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Test samples: {len(test_df)}\")\n",
    "    print(f\"Features: {len(train_df.columns) - 1}\")\n",
    "    print(f\"Classes: {train_df['label'].nunique()}\")\n",
    "    \n",
    "    return train_processed, test_processed\n",
    "\n",
    "\n",
    "def create_ludwig_config_basic():\n",
    "    \"\"\"Create a basic Ludwig configuration for MNIST.\"\"\"\n",
    "    config = {\n",
    "        'input_features': [\n",
    "            {\n",
    "                'name': 'pixel_features',\n",
    "                'type': 'number',\n",
    "                'preprocessing': {\n",
    "                    'normalization': 'zscore'\n",
    "                },\n",
    "                'encoder': {\n",
    "                    'type': 'dense',\n",
    "                    'layers': [\n",
    "                        {'output_size': 256, 'activation': 'relu'},\n",
    "                        {'output_size': 128, 'activation': 'relu'}\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'output_features': [\n",
    "            {\n",
    "                'name': 'label',\n",
    "                'type': 'category',\n",
    "                'decoder': {\n",
    "                    'type': 'classifier',\n",
    "                    'num_fc_layers': 2,\n",
    "                    'output_size': 64\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'trainer': {\n",
    "            'epochs': 10,\n",
    "            'batch_size': 128,\n",
    "            'learning_rate': 0.001,\n",
    "            'early_stop': 5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(DATA_DIR, 'config_basic.yaml')\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    print(f\"\\nBasic configuration saved to {config_path}\")\n",
    "    return config\n",
    "\n",
    "\n",
    "def create_ludwig_config_advanced():\n",
    "    \"\"\"Create an advanced Ludwig configuration with CNN-like architecture.\"\"\"\n",
    "    # Since Ludwig expects flattened features for number type,\n",
    "    # we'll use a more complex dense architecture\n",
    "    config = {\n",
    "        'input_features': [\n",
    "            {\n",
    "                'name': f'pixel_{i}',\n",
    "                'type': 'number',\n",
    "                'preprocessing': {\n",
    "                    'normalization': 'minmax'  # Normalize pixels to [0, 1]\n",
    "                }\n",
    "            } for i in range(784)\n",
    "        ],\n",
    "        'combiner': {\n",
    "            'type': 'concat',\n",
    "            'num_fc_layers': 3,\n",
    "            'output_size': 256,\n",
    "            'fc_layers': [\n",
    "                {'output_size': 512, 'activation': 'relu', 'dropout': 0.2},\n",
    "                {'output_size': 256, 'activation': 'relu', 'dropout': 0.2},\n",
    "                {'output_size': 128, 'activation': 'relu', 'dropout': 0.1}\n",
    "            ]\n",
    "        },\n",
    "        'output_features': [\n",
    "            {\n",
    "                'name': 'label',\n",
    "                'type': 'category',\n",
    "                'decoder': {\n",
    "                    'type': 'classifier',\n",
    "                    'num_fc_layers': 1,\n",
    "                    'output_size': 64\n",
    "                },\n",
    "                'loss': {\n",
    "                    'type': 'softmax_cross_entropy'\n",
    "                }\n",
    "            }\n",
    "        ],\n",
    "        'trainer': {\n",
    "            'epochs': 15,\n",
    "            'batch_size': 256,\n",
    "            'learning_rate': 0.001,\n",
    "            'learning_rate_scheduler': {\n",
    "                'type': 'reduce_on_plateau',\n",
    "                'factor': 0.5,\n",
    "                'patience': 3\n",
    "            },\n",
    "            'early_stop': 5,\n",
    "            'optimizer': {\n",
    "                'type': 'adam'\n",
    "            },\n",
    "            'validation_metric': 'accuracy'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    config_path = os.path.join(DATA_DIR, 'config_advanced.yaml')\n",
    "    with open(config_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "    \n",
    "    print(f\"Advanced configuration saved to {config_path}\")\n",
    "    return config\n",
    "\n",
    "\n",
    "def train_model(config, train_file, test_file, model_name=\"basic\"):\n",
    "    \"\"\"Train a Ludwig model with the given configuration.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {model_name.upper()} model...\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Initialize Ludwig model\n",
    "    model = LudwigModel(config=config, logging_level='INFO')\n",
    "    \n",
    "    # Train the model\n",
    "    train_stats, preprocessed_data, output_directory = model.train(\n",
    "        dataset=train_file,\n",
    "        test=test_file,\n",
    "        experiment_name=f'mnist_{model_name}',\n",
    "        model_name=f'mnist_model_{model_name}'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nModel trained and saved to: {output_directory}\")\n",
    "    return model, train_stats, output_directory\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_file):\n",
    "    \"\"\"Evaluate the model on the test set.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Evaluating model on test set...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_stats, predictions, output_directory = model.evaluate(\n",
    "        dataset=test_file,\n",
    "        collect_predictions=True,\n",
    "        collect_overall_stats=True\n",
    "    )\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(\"\\nTest Set Performance:\")\n",
    "    print(\"-\" * 40)\n",
    "    for metric, value in eval_stats['label'].items():\n",
    "        if isinstance(value, (int, float)):\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    return eval_stats, predictions\n",
    "\n",
    "\n",
    "def visualize_results(model, test_file, output_dir):\n",
    "    \"\"\"Generate visualizations of model performance.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Generating visualizations...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    try:\n",
    "        from ludwig.visualize import learning_curves, confusion_matrix, compare_performance\n",
    "        \n",
    "        # Learning curves\n",
    "        print(\"Creating learning curves...\")\n",
    "        learning_curves([output_dir], output_directory=output_dir)\n",
    "        \n",
    "        # Confusion matrix\n",
    "        print(\"Creating confusion matrix...\")\n",
    "        confusion_matrix([output_dir], test_file, \n",
    "                        'label', output_directory=output_dir)\n",
    "        \n",
    "        print(f\"\\nVisualizations saved to: {output_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Visualization error (optional): {e}\")\n",
    "        print(\"Install matplotlib for visualizations: pip install matplotlib\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"MNIST Classification with Ludwig\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Download data\n",
    "    print(\"\\nStep 1: Downloading data...\")\n",
    "    download_data()\n",
    "    \n",
    "    # Step 2: Prepare data\n",
    "    print(\"\\nStep 2: Preparing data...\")\n",
    "    train_file, test_file = prepare_data()\n",
    "    \n",
    "    # Step 3: Choose configuration\n",
    "    print(\"\\nStep 3: Creating model configuration...\")\n",
    "    print(\"\\nChoose model configuration:\")\n",
    "    print(\"1. Basic (faster, simpler architecture)\")\n",
    "    print(\"2. Advanced (better performance, more complex)\")\n",
    "    \n",
    "    choice = input(\"\\nEnter choice (1 or 2, default=1): \").strip() or \"1\"\n",
    "    \n",
    "    if choice == \"2\":\n",
    "        config = create_ludwig_config_advanced()\n",
    "        model_name = \"advanced\"\n",
    "    else:\n",
    "        config = create_ludwig_config_basic()\n",
    "        model_name = \"basic\"\n",
    "    \n",
    "    # Step 4: Train model\n",
    "    model, train_stats, output_dir = train_model(\n",
    "        config, train_file, test_file, model_name\n",
    "    )\n",
    "    \n",
    "    # Step 5: Evaluate model\n",
    "    eval_stats, predictions = evaluate_model(model, test_file)\n",
    "    \n",
    "    # Step 6: Visualize results\n",
    "    visualize_results(model, test_file, output_dir)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Training and evaluation complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nModel saved to: {output_dir}\")\n",
    "    print(f\"You can load this model later using:\")\n",
    "    print(f\"  from ludwig.api import LudwigModel\")\n",
    "    print(f\"  model = LudwingModel.load('{output_dir}')\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebfde9-88eb-4e71-a20f-7af2f4755657",
   "metadata": {},
   "source": [
    "The code is running but I was not able to install ludwig after so many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d058dcc-5c30-4e24-b773-5ed9b96d1a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
