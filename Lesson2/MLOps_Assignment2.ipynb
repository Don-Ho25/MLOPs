{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a00c73a6-e13d-41d0-b2f4-b30cd6e01279",
   "metadata": {},
   "source": [
    "\n",
    "## Assignment 2: Containerization and Edge Computing\n",
    "\n",
    "**DONG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d22c856d-8b5b-4be4-ac34-3cbb95b84821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages installed successfully!\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages\n",
    "# !pip install flask tensorflow pillow numpy requests\n",
    "\n",
    "# Verify installations\n",
    "import flask\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "print(\"‚úÖ All packages installed successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb51466-a5d1-4f2f-89e2-8c5d693ae82e",
   "metadata": {},
   "source": [
    "## Build the enhanced Flask app  in Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e20e734-69f4-4cb1-a7f2-6120615c004f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import json\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd474b1b-3b4e-446e-b8f3-73e0582b5a10",
   "metadata": {},
   "source": [
    "Create Sample TensorFlow Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb567f35-560a-4fa8-824d-3e4bb666db8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "INFO:absl:Function `function` contains input name(s) resource with unsupported characters which will be renamed to sequential_1_dense_1_biasadd_readvariableop_resource in the SavedModel.\n",
      "INFO:absl:Function `function` contains input name(s) resource with unsupported characters which will be renamed to sequential_1_dense_1_biasadd_readvariableop_resource in the SavedModel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\tmpzfo5l3ha\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\LENOVO\\AppData\\Local\\Temp\\tmpzfo5l3ha\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\LENOVO\\AppData\\Local\\Temp\\tmpzfo5l3ha'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 10), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  1808787770256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1808787771600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1808787771216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  1808787771984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "‚úÖ Demo TFLite model created: model.tflite\n"
     ]
    }
   ],
   "source": [
    "# Create a simple model for testing\n",
    "def create_demo_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Convert to TensorFlow Lite\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    # Save model\n",
    "    with open('model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    return tflite_model\n",
    "\n",
    "# Create the model\n",
    "tflite_model = create_demo_model()\n",
    "print(\"‚úÖ Demo TFLite model created: model.tflite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc20648-a0fd-4d2f-b60b-bf45148965f9",
   "metadata": {},
   "source": [
    "Load TensorFlow Lite Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ac72fe2-a4cf-45f7-bdd7-2c2941b44143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TensorFlow Lite model loaded successfully!\n",
      "Input shape: [  1 224 224   3]\n",
      "Input type: <class 'numpy.float32'>\n",
      "Output shape: [ 1 10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\\tensorflow\\lite\\python\\interpreter.py:457: UserWarning:     Warning: tf.lite.Interpreter is deprecated and is scheduled for deletion in\n",
      "    TF 2.20. Please use the LiteRT interpreter from the ai_edge_litert package.\n",
      "    See the [migration guide](https://ai.google.dev/edge/litert/migration)\n",
      "    for details.\n",
      "    \n",
      "  warnings.warn(_INTERPRETER_DELETION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path='model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get model details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "print(\"‚úÖ TensorFlow Lite model loaded successfully!\")\n",
    "print(f\"Input shape: {input_details[0]['shape']}\")\n",
    "print(f\"Input type: {input_details[0]['dtype']}\")\n",
    "print(f\"Output shape: {output_details[0]['shape']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c363f022-e6e0-41fb-9f81-ccd4fa5c17df",
   "metadata": {},
   "source": [
    "Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ef023c5-deff-4c99-bb99-819263125698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Image processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "def decode_base64_image(base64_string):\n",
    "    \"\"\"Decode base64 encoded image string\"\"\"\n",
    "    try:\n",
    "        if ',' in base64_string:\n",
    "            base64_string = base64_string.split(',')[1]\n",
    "        \n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        return image_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error decoding base64 image: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_image(image_data, target_size=(224, 224)):\n",
    "    \"\"\"Preprocess image for model inference\"\"\"\n",
    "    try:\n",
    "        # Open image from bytes\n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        \n",
    "        # Convert to RGB if necessary\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Resize image\n",
    "        image = image.resize(target_size)\n",
    "        \n",
    "        # Convert to numpy array and normalize\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Add batch dimension\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        \n",
    "        return image_array\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preprocessing image: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"‚úÖ Image processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967650c3-2993-41f9-9c24-0731a9813b43",
   "metadata": {},
   "source": [
    "Image Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2957cb33-4efa-4d6a-a423-cbd606bdaf24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Image processing functions defined!\n"
     ]
    }
   ],
   "source": [
    "def decode_base64_image(base64_string):\n",
    "    \"\"\"Decode base64 encoded image string\"\"\"\n",
    "    try:\n",
    "        if ',' in base64_string:\n",
    "            base64_string = base64_string.split(',')[1]\n",
    "        \n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        return image_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error decoding base64 image: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_image(image_data, target_size=(224, 224)):\n",
    "    \"\"\"Preprocess image for model inference\"\"\"\n",
    "    try:\n",
    "        # Open image from bytes\n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        \n",
    "        # Convert to RGB if necessary\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        \n",
    "        # Resize image\n",
    "        image = image.resize(target_size)\n",
    "        \n",
    "        # Convert to numpy array and normalize\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Add batch dimension\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        \n",
    "        return image_array\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preprocessing image: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"‚úÖ Image processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b2d8e-d9c3-4d7c-9ffe-3cb479aabc4d",
   "metadata": {},
   "source": [
    "Prediction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75c5d593-b4f1-4982-8883-1db2dabef82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prediction function defined!\n"
     ]
    }
   ],
   "source": [
    "def predict_with_tflite(preprocessed_image):\n",
    "    \"\"\"Perform inference using TensorFlow Lite model\"\"\"\n",
    "    try:\n",
    "        # Get input and output tensors\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        \n",
    "        # Set input tensor\n",
    "        interpreter.set_tensor(input_details[0]['index'], preprocessed_image)\n",
    "        \n",
    "        # Run inference\n",
    "        interpreter.invoke()\n",
    "        \n",
    "        # Get prediction results\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        \n",
    "        # Process output\n",
    "        predictions = {\n",
    "            'predictions': output_data.tolist(),\n",
    "            'predicted_class': int(np.argmax(output_data)),\n",
    "            'confidence': float(np.max(output_data)),\n",
    "            'all_confidences': [float(x) for x in output_data[0]]\n",
    "        }\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during inference: {e}\")\n",
    "        raise\n",
    "\n",
    "print(\"‚úÖ Prediction function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f191d0f-c05c-488c-9916-5b8e3af96ec6",
   "metadata": {},
   "source": [
    "Create Enhanced Flask App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46cb32f6-800a-4536-9a46-202da37b64dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Basic Flask app created!\n"
     ]
    }
   ],
   "source": [
    "# Create Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"\"\"\n",
    "    <h1>üöÄ Enhanced Flask App with TensorFlow Lite</h1>\n",
    "    <p>Endpoints available:</p>\n",
    "    <ul>\n",
    "        <li><a href=\"/health\">/health</a> - Health check</li>\n",
    "        <li>/predict - POST endpoint for image prediction</li>\n",
    "        <li>/process-image - POST endpoint for image processing</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'model_loaded': True,\n",
    "        'endpoints': ['/health', '/predict', '/process-image']\n",
    "    })\n",
    "\n",
    "print(\"‚úÖ Basic Flask app created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb05bf1-0fb4-40bc-8c66-d5a3ad0252bf",
   "metadata": {},
   "source": [
    "Add Image Processing Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e44fc2a-c29d-4018-983a-d24d4893f95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Image processing endpoint added!\n"
     ]
    }
   ],
   "source": [
    "@app.route('/process-image', methods=['POST'])\n",
    "def process_image():\n",
    "    \"\"\"Endpoint for image processing without prediction\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        if not data or 'image' not in data:\n",
    "            return jsonify({'error': 'No image data provided'}), 400\n",
    "        \n",
    "        # Decode base64 image\n",
    "        image_data = decode_base64_image(data['image'])\n",
    "        \n",
    "        # Preprocess image\n",
    "        processed_image = preprocess_image(image_data)\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'message': 'Image processed successfully',\n",
    "            'image_shape': processed_image.shape,\n",
    "            'data_range': {\n",
    "                'min': float(processed_image.min()),\n",
    "                'max': float(processed_image.max())\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Image processing error: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "print(\"‚úÖ Image processing endpoint added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b04f0e-74ea-4e57-ad05-4cbc88a99738",
   "metadata": {},
   "source": [
    "Add Prediction Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3ae0ae4-de60-44be-ab58-0c3720fb14d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prediction endpoint added!\n"
     ]
    }
   ],
   "source": [
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"Endpoint for image prediction\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        \n",
    "        if not data or 'image' not in data:\n",
    "            return jsonify({'error': 'No image data provided'}), 400\n",
    "        \n",
    "        logger.info(\"Received prediction request\")\n",
    "        \n",
    "        # Decode base64 image\n",
    "        image_data = decode_base64_image(data['image'])\n",
    "        \n",
    "        # Preprocess image\n",
    "        processed_image = preprocess_image(image_data)\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = predict_with_tflite(processed_image)\n",
    "        \n",
    "        logger.info(f\"Prediction completed: class {predictions['predicted_class']}\")\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'predictions': predictions\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "print(\"‚úÖ Prediction endpoint added!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fcee13-b06a-402b-94da-d65b3d0a6d2c",
   "metadata": {},
   "source": [
    "Test Image Creation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb77a447-7e71-433e-a540-9854186a88a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test image created!\n",
      "Base64 length: 40927\n"
     ]
    }
   ],
   "source": [
    "def create_test_image_base64(width=224, height=224):\n",
    "    \"\"\"Create a test image and return as base64\"\"\"\n",
    "    # Create a random image\n",
    "    img_array = np.random.randint(0, 255, (height, width, 3), dtype=np.uint8)\n",
    "    img = Image.fromarray(img_array)\n",
    "    \n",
    "    # Convert to base64\n",
    "    buffered = io.BytesIO()\n",
    "    img.save(buffered, format=\"JPEG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "    \n",
    "    return f\"data:image/jpeg;base64,{img_str}\"\n",
    "\n",
    "# Create test image\n",
    "test_image = create_test_image_base64()\n",
    "print(\"‚úÖ Test image created!\")\n",
    "print(f\"Base64 length: {len(test_image)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27123b09-dcff-4c40-8872-f21851b98ada",
   "metadata": {},
   "source": [
    "Run and Test the Complete App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9574a05-8498-4e1b-9961-2ce94a998ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.2.17:5000\n",
      "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Flask app started on http://localhost:5000\n",
      "Testing endpoints...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [30/Nov/2025 00:15:36] \"GET /health HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Health check: 200 - {'endpoints': ['/health', '/predict', '/process-image'], 'model_loaded': True, 'status': 'healthy'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Received prediction request\n",
      "INFO:__main__:Prediction completed: class 3\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Nov/2025 00:15:38] \"POST /predict HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Prediction test: 200\n",
      "   Predicted class: 3\n",
      "   Confidence: 0.1113\n",
      "\n",
      "üéâ Enhanced Flask App is running!\n",
      "You can now test it in your browser: http://localhost:5000\n",
      "Or send POST requests to /predict and /process-image\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:werkzeug:127.0.0.1 - - [30/Nov/2025 00:16:25] \"GET /health HTTP/1.1\" 200 -\n",
      "INFO:werkzeug:127.0.0.1 - - [30/Nov/2025 00:16:27] \"GET / HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def run_flask_app():\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "# Start Flask app in background\n",
    "flask_thread = threading.Thread(target=run_flask_app)\n",
    "flask_thread.daemon = True\n",
    "flask_thread.start()\n",
    "\n",
    "# Wait for app to start\n",
    "time.sleep(3)\n",
    "\n",
    "print(\"üöÄ Flask app started on http://localhost:5000\")\n",
    "print(\"Testing endpoints...\")\n",
    "\n",
    "# Test health endpoint\n",
    "try:\n",
    "    health_response = requests.get('http://localhost:5000/health')\n",
    "    print(f\"‚úÖ Health check: {health_response.status_code} - {health_response.json()}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Health check failed: {e}\")\n",
    "\n",
    "# Test prediction endpoint\n",
    "try:\n",
    "    test_data = {'image': test_image}\n",
    "    predict_response = requests.post('http://localhost:5000/predict', json=test_data)\n",
    "    print(f\"‚úÖ Prediction test: {predict_response.status_code}\")\n",
    "    if predict_response.status_code == 200:\n",
    "        result = predict_response.json()\n",
    "        print(f\"   Predicted class: {result['predictions']['predicted_class']}\")\n",
    "        print(f\"   Confidence: {result['predictions']['confidence']:.4f}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Prediction test failed: {e}\")\n",
    "\n",
    "print(\"\\nüéâ Enhanced Flask App is running!\")\n",
    "print(\"You can now test it in your browser: http://localhost:5000\")\n",
    "print(\"Or send POST requests to /predict and /process-image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36275794-f524-4994-95e0-e337db616268",
   "metadata": {},
   "source": [
    "## Create Deployment Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a97a2-5016-4e34-b572-a7cfe9d9dbf1",
   "metadata": {},
   "source": [
    "Create Requirements file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b615de07-7893-46e7-a006-fde77894883c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ requirements.txt created\n"
     ]
    }
   ],
   "source": [
    "requirements = \"\"\"\n",
    "flask==2.3.3\n",
    "tensorflow==2.13.0\n",
    "pillow==10.0.0\n",
    "numpy==1.24.3\n",
    "gunicorn==21.2.0\n",
    "\"\"\"\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "print(\"‚úÖ requirements.txt created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9eac6f3-6b86-4cee-8e76-e921e8ea55d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "flask==2.3.3\n",
      "tensorflow==2.13.0\n",
      "pillow==10.0.0\n",
      "numpy==1.24.3\n",
      "gunicorn==21.2.0\n"
     ]
    }
   ],
   "source": [
    "!type requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4c1e39-d77f-4251-b5d3-3377a9f98707",
   "metadata": {},
   "source": [
    "Create Dockerfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ff0ebd5-25f6-4492-b6c3-4ab4f42c58c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dockerfile created\n"
     ]
    }
   ],
   "source": [
    "# Create Dockerfile for deployment\n",
    "dockerfile_content = \"\"\"\n",
    "# Use Python 3.9 slim image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Install system dependencies\n",
    "RUN apt-get update && apt-get install -y \\\\\n",
    "    gcc \\\\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy requirements and install Python dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy application code and model\n",
    "COPY app.py .\n",
    "COPY model.tflite .\n",
    "\n",
    "# Create non-root user for security\n",
    "RUN useradd --create-home --shell /bin/bash app\n",
    "USER app\n",
    "\n",
    "# Expose port\n",
    "EXPOSE 5000\n",
    "\n",
    "# Run the application\n",
    "CMD [\"python\", \"app.py\"]\n",
    "\"\"\"\n",
    "\n",
    "with open('Dockerfile', 'w') as f:\n",
    "    f.write(dockerfile_content)\n",
    "\n",
    "print(\"‚úÖ Dockerfile created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf047a7-da36-4b92-8e21-f6b7128430fd",
   "metadata": {},
   "source": [
    "Create Standalone App File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db916c2c-3269-4834-b2c8-d1fd85ea6c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Clean app file created: app_clean.py\n"
     ]
    }
   ],
   "source": [
    "# Let's create a clean version of app.py without invisible characters\n",
    "clean_app_code = '''from flask import Flask, request, jsonify\n",
    "import base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "try:\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    logger.info(\"TensorFlow Lite model loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model: {e}\")\n",
    "    interpreter = None\n",
    "\n",
    "def decode_base64_image(base64_string):\n",
    "    \"\"\"Decode base64 encoded image string\"\"\"\n",
    "    try:\n",
    "        if ',' in base64_string:\n",
    "            base64_string = base64_string.split(',')[1]\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        return image_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error decoding base64 image: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_image(image_data, target_size=(224, 224)):\n",
    "    \"\"\"Preprocess image for model inference\"\"\"\n",
    "    try:\n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        image = image.resize(target_size)\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        return image_array\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preprocessing image: {e}\")\n",
    "        raise\n",
    "\n",
    "def predict_with_tflite(preprocessed_image):\n",
    "    \"\"\"Perform inference using TensorFlow Lite model\"\"\"\n",
    "    try:\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        interpreter.set_tensor(input_details[0]['index'], preprocessed_image)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions = {\n",
    "            'predictions': output_data.tolist(),\n",
    "            'predicted_class': int(np.argmax(output_data)),\n",
    "            'confidence': float(np.max(output_data)),\n",
    "            'all_confidences': [float(x) for x in output_data[0]]\n",
    "        }\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during inference: {e}\")\n",
    "        raise\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"\"\"\n",
    "    <h1>Enhanced Flask App with TensorFlow Lite</h1>\n",
    "    <p>Endpoints available:</p>\n",
    "    <ul>\n",
    "        <li><a href=\"/health\">/health</a> - Health check</li>\n",
    "        <li>/predict - POST endpoint for image prediction</li>\n",
    "        <li>/process-image - POST endpoint for image processing</li>\n",
    "    </ul>\n",
    "    \"\"\"\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'model_loaded': interpreter is not None,\n",
    "        'endpoints': ['/health', '/predict', '/process-image']\n",
    "    })\n",
    "\n",
    "@app.route('/process-image', methods=['POST'])\n",
    "def process_image():\n",
    "    \"\"\"Endpoint for image processing without prediction\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if not data or 'image' not in data:\n",
    "            return jsonify({'error': 'No image data provided'}), 400\n",
    "        image_data = decode_base64_image(data['image'])\n",
    "        processed_image = preprocess_image(image_data)\n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'message': 'Image processed successfully',\n",
    "            'image_shape': processed_image.shape,\n",
    "            'data_range': {\n",
    "                'min': float(processed_image.min()),\n",
    "                'max': float(processed_image.max())\n",
    "            }\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Image processing error: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"Endpoint for image prediction\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if not data or 'image' not in data:\n",
    "            return jsonify({'error': 'No image data provided'}), 400\n",
    "        image_data = decode_base64_image(data['image'])\n",
    "        processed_image = preprocess_image(image_data)\n",
    "        predictions = predict_with_tflite(processed_image)\n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'predictions': predictions\n",
    "        })\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
    "'''\n",
    "\n",
    "# Write the clean version\n",
    "with open('app_clean.py', 'w') as f:\n",
    "    f.write(clean_app_code)\n",
    "\n",
    "print(\"‚úÖ Clean app file created: app_clean.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a229df66-fd83-4616-ab47-20601e451503",
   "metadata": {},
   "source": [
    "## Test and Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3c0afd-0df1-4c59-a757-50a2f1100e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the clean version\n",
    "print(\"Testing clean app version...\")\n",
    "!python app_clean.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1df77702-70d2-44d4-87c0-622e05cf64c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current app.py content:\n",
      "==================================================\n",
      "from flask import Flask, request, jsonify\n",
      "import base64\n",
      "import numpy as np\n",
      "from PIL import Image\n",
      "import io\n",
      "import tensorflow as tf\n",
      "import logging\n",
      "\n",
      "# Set up logging\n",
      "logging.basicConfig(level=logging.INFO)\n",
      "logger = logging.getLogger(__name__)\n",
      "\n",
      "app = Flask(__name__)\n",
      "\n",
      "# Load TensorFlow Lite model\n",
      "try:\n",
      "    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
      "    interpreter.allocate_tensors()\n",
      "    input_details = interpreter.get_input_details()\n",
      "    output_details = interpreter.get_output_details()\n",
      "    logger.info(\"TensorFlow Lite model loaded successfully\")\n",
      "except Exception as e:\n",
      "    logger.error(f\"Error loading model: {e}\")\n",
      "    interpreter = None\n",
      "\n",
      "def decode_base64_image(base64_string):\n",
      "    \"\"\"Decode base64 encoded image string\"\"\"\n",
      "    try:\n",
      "        if ',' in base64_string:\n",
      "            base64_string = base64_string.split(',')[1]\n",
      "        image_data = base64.b64decode(base64_string)\n",
      "        return image_data\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error decoding base64 image: {e}\")\n",
      "        raise\n",
      "\n",
      "def preprocess_image(image_data, target_size=(224, 224)):\n",
      "    \"\"\"Preprocess image for model inference\"\"\"\n",
      "    try:\n",
      "        image = Image.open(io.BytesIO(image_data))\n",
      "        if image.mode != 'RGB':\n",
      "            image = image.convert('RGB')\n",
      "        image = image.resize(target_size)\n",
      "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
      "        image_array = np.expand_dims(image_array, axis=0)\n",
      "        return image_array\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error preprocessing image: {e}\")\n",
      "        raise\n",
      "\n",
      "def predict_with_tflite(preprocessed_image):\n",
      "    \"\"\"Perform inference using TensorFlow Lite model\"\"\"\n",
      "    try:\n",
      "        input_details = interpreter.get_input_details()\n",
      "        output_details = interpreter.get_output_details()\n",
      "        interpreter.set_tensor(input_details[0]['index'], preprocessed_image)\n",
      "        interpreter.invoke()\n",
      "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
      "        predictions = {\n",
      "            'predictions': output_data.tolist(),\n",
      "            'predicted_class': int(np.argmax(output_data)),\n",
      "            'confidence': float(np.max(output_data)),\n",
      "            'all_confidences': [float(x) for x in output_data[0]]\n",
      "        }\n",
      "        return predictions\n",
      "    except Exception as e:\n",
      "        logger.error(f\"Error during inference: {e}\")\n",
      "        raise\n",
      "\n",
      "@app.route('/')\n",
      "def home():\n",
      "    return \"Flask App is Running!\"\n",
      "\n",
      "@app.route('/health', methods=['GET'])\n",
      "def health_check():\n",
      "    return jsonify({\n",
      "        'status': 'healthy',\n",
      "        'model_loaded': interpreter is not None,\n",
      "        'endpoints': ['/health', '/predict', '/process-image']\n",
      "    })\n",
      "\n",
      "@app.route('/process-image', methods=['POST'])\n",
      "def process_image():\n",
      "    \"\"\"Endpoint for image processing without prediction\"\"\"\n",
      "    try:\n",
      "        data = request.get_json()\n",
      "        if not data or 'image' not in data:\n",
      "            return jsonify({'error': 'No image data provided'}), 400\n",
      "        \n",
      "        # Decode base64 image\n",
      "        image_data = decode_base64_image(data['image'])\n",
      "        \n",
      "        # Preprocess image\n",
      "        processed_image = preprocess_image(image_data)\n",
      "        \n",
      "        return jsonify({\n",
      "            'status': 'success',\n",
      "            'message': 'Image processed successfully',\n",
      "            'image_shape': processed_image.shape,\n",
      "            'data_range': {\n",
      "                'min': float(processed_image.min()),\n",
      "                'max': float(processed_image.max())\n",
      "            }\n",
      "        })\n",
      "        \n",
      "    except Exception as e:\n",
      "        logger.error(f\"Image processing error: {e}\")\n",
      "        return jsonify({'error': str(e)}), 500\n",
      "\n",
      "@app.route('/predict', methods=['POST'])\n",
      "def predict():\n",
      "    \"\"\"Endpoint for image prediction\"\"\"\n",
      "    try:\n",
      "        data = request.get_json()\n",
      "        if not data or 'image' not in data:\n",
      "            return jsonify({'error': 'No image data provided'}), 400\n",
      "        \n",
      "        logger.info(\"Received prediction request\")\n",
      "        \n",
      "        # Decode base64 image\n",
      "        image_data = decode_base64_image(data['image'])\n",
      "        \n",
      "        # Preprocess image\n",
      "        processed_image = preprocess_image(image_data)\n",
      "        \n",
      "        # Make prediction\n",
      "        predictions = predict_with_tflite(processed_image)\n",
      "        \n",
      "        logger.info(f\"Prediction completed: class {predictions['predicted_class']}\")\n",
      "        \n",
      "        return jsonify({\n",
      "            'status': 'success',\n",
      "            'predictions': predictions\n",
      "        })\n",
      "        \n",
      "    except Exception as e:\n",
      "        logger.error(f\"Prediction error: {e}\")\n",
      "        return jsonify({'error': str(e)}), 500\n",
      "\n",
      "if __name__ == '__main__':\n",
      "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Check what's in your app.py file\n",
    "print(\"Current app.py content:\")\n",
    "print(\"=\" * 50)\n",
    "with open('app.py', 'r') as f:\n",
    "    content = f.read()\n",
    "    print(content)\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc4fd6b6-b2b5-440e-be94-d032847889dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Enhanced Flask app created successfully!\n",
      "üìÅ File: app.py\n",
      "üöÄ Ready to run with all endpoints!\n"
     ]
    }
   ],
   "source": [
    "# Create the correct enhanced Flask app\n",
    "enhanced_app_code = '''from flask import Flask, request, jsonify\n",
    "import base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load TensorFlow Lite model\n",
    "try:\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    logger.info(\"TensorFlow Lite model loaded successfully\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error loading model: {e}\")\n",
    "    interpreter = None\n",
    "\n",
    "def decode_base64_image(base64_string):\n",
    "    \"\"\"Decode base64 encoded image string\"\"\"\n",
    "    try:\n",
    "        if ',' in base64_string:\n",
    "            base64_string = base64_string.split(',')[1]\n",
    "        image_data = base64.b64decode(base64_string)\n",
    "        return image_data\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error decoding base64 image: {e}\")\n",
    "        raise\n",
    "\n",
    "def preprocess_image(image_data, target_size=(224, 224)):\n",
    "    \"\"\"Preprocess image for model inference\"\"\"\n",
    "    try:\n",
    "        image = Image.open(io.BytesIO(image_data))\n",
    "        if image.mode != 'RGB':\n",
    "            image = image.convert('RGB')\n",
    "        image = image.resize(target_size)\n",
    "        image_array = np.array(image, dtype=np.float32) / 255.0\n",
    "        image_array = np.expand_dims(image_array, axis=0)\n",
    "        return image_array\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error preprocessing image: {e}\")\n",
    "        raise\n",
    "\n",
    "def predict_with_tflite(preprocessed_image):\n",
    "    \"\"\"Perform inference using TensorFlow Lite model\"\"\"\n",
    "    try:\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        interpreter.set_tensor(input_details[0]['index'], preprocessed_image)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        predictions = {\n",
    "            'predictions': output_data.tolist(),\n",
    "            'predicted_class': int(np.argmax(output_data)),\n",
    "            'confidence': float(np.max(output_data)),\n",
    "            'all_confidences': [float(x) for x in output_data[0]]\n",
    "        }\n",
    "        return predictions\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error during inference: {e}\")\n",
    "        raise\n",
    "\n",
    "@app.route('/')\n",
    "def home():\n",
    "    return \"Flask App is Running!\"\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health_check():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'model_loaded': interpreter is not None,\n",
    "        'endpoints': ['/health', '/predict', '/process-image']\n",
    "    })\n",
    "\n",
    "@app.route('/process-image', methods=['POST'])\n",
    "def process_image():\n",
    "    \"\"\"Endpoint for image processing without prediction\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if not data or 'image' not in data:\n",
    "            return jsonify({'error': 'No image data provided'}), 400\n",
    "        \n",
    "        # Decode base64 image\n",
    "        image_data = decode_base64_image(data['image'])\n",
    "        \n",
    "        # Preprocess image\n",
    "        processed_image = preprocess_image(image_data)\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'message': 'Image processed successfully',\n",
    "            'image_shape': processed_image.shape,\n",
    "            'data_range': {\n",
    "                'min': float(processed_image.min()),\n",
    "                'max': float(processed_image.max())\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Image processing error: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    \"\"\"Endpoint for image prediction\"\"\"\n",
    "    try:\n",
    "        data = request.get_json()\n",
    "        if not data or 'image' not in data:\n",
    "            return jsonify({'error': 'No image data provided'}), 400\n",
    "        \n",
    "        logger.info(\"Received prediction request\")\n",
    "        \n",
    "        # Decode base64 image\n",
    "        image_data = decode_base64_image(data['image'])\n",
    "        \n",
    "        # Preprocess image\n",
    "        processed_image = preprocess_image(image_data)\n",
    "        \n",
    "        # Make prediction\n",
    "        predictions = predict_with_tflite(processed_image)\n",
    "        \n",
    "        logger.info(f\"Prediction completed: class {predictions['predicted_class']}\")\n",
    "        \n",
    "        return jsonify({\n",
    "            'status': 'success',\n",
    "            'predictions': predictions\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction error: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, debug=False)\n",
    "'''\n",
    "\n",
    "# Write the enhanced app\n",
    "with open('app.py', 'w') as f:\n",
    "    f.write(enhanced_app_code)\n",
    "\n",
    "print(\"‚úÖ Enhanced Flask app created successfully!\")\n",
    "print(\"üìÅ File: app.py\")\n",
    "print(\"üöÄ Ready to run with all endpoints!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a89b0be1-1915-4ca1-b9d2-f2d4f93c85be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model file found: model.tflite (6832 bytes)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if model file exists\n",
    "if os.path.exists('model.tflite'):\n",
    "    file_size = os.path.getsize('model.tflite')\n",
    "    print(f\"‚úÖ Model file found: model.tflite ({file_size} bytes)\")\n",
    "else:\n",
    "    print(\"‚ùå Model file not found! Let's create it...\")\n",
    "    \n",
    "    # Create a simple model if missing\n",
    "    import tensorflow as tf\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
    "        tf.keras.layers.GlobalAveragePooling2D(),\n",
    "        tf.keras.layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    \n",
    "    with open('model.tflite', 'wb') as f:\n",
    "        f.write(tflite_model)\n",
    "    \n",
    "    print(\"‚úÖ Demo model created: model.tflite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deedda52-ee4c-46f3-b425-9d06be72ea97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Enhanced Flask App...\n",
      "All endpoints available:\n",
      "‚Ä¢ http://localhost:5000/ - Home\n",
      "‚Ä¢ http://localhost:5000/health - Health check\n",
      "‚Ä¢ POST http://localhost:5000/predict - Image prediction\n",
      "‚Ä¢ POST http://localhost:5000/process-image - Image processing\n",
      "\n",
      "App is running... (Click Stop button when done)\n"
     ]
    }
   ],
   "source": [
    "# Run the enhanced Flask app\n",
    "print(\"Starting Enhanced Flask App...\")\n",
    "print(\"All endpoints available:\")\n",
    "print(\"‚Ä¢ http://localhost:5000/ - Home\")\n",
    "print(\"‚Ä¢ http://localhost:5000/health - Health check\") \n",
    "print(\"‚Ä¢ POST http://localhost:5000/predict - Image prediction\")\n",
    "print(\"‚Ä¢ POST http://localhost:5000/process-image - Image processing\")\n",
    "print(\"\\nApp is running... (Click Stop button when done)\")\n",
    "\n",
    "!python app.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44be30b6-e434-428b-b92a-a860dbe273cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555e2ad9-51af-4a0e-97d0-9819d4a7398a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
