{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc0e81bd-8fe9-4479-8702-552f73f7cc09",
   "metadata": {},
   "source": [
    "\n",
    "## Assignment 5: Monitoring and Logging in DevOps and MLOps\n",
    "\n",
    "\n",
    "**DONG**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2abaaf3-c4eb-45f2-9dc7-b9b1c7f63ddf",
   "metadata": {},
   "source": [
    "#### Part 1: Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef21566-5683-448a-bb21-7c1864543ec7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### 1.    Explain the importance of logging in a machine learning deployment pipeline. How does it contribute to the overall reliability of the system?\n",
    "\n",
    "Logging in a machine learning deployment pipeline serves as a comprehensive record of all system activities, decisions, and errors that occur during model inference and operations. It captures critical information such as input data characteristics, model predictions, processing times, and any exceptions or failures that occur. This detailed record-keeping is essential for debugging issues when they arise, understanding model behavior in production, and conducting post-mortem analyses after incidents. Logging contributes to system reliability by providing visibility into the model's operations, enabling quick identification of problems, facilitating root cause analysis, and supporting compliance requirements. When something goes wrong, logs are often the first place engineers look to understand what happened, when it happened, and why it happened, making them indispensable for maintaining stable production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d42a88-0f88-4b26-904f-1bce853bd10d",
   "metadata": {},
   "source": [
    "##### 2.   What is data drift in the context of machine learning models? Why is it crucial to monitor for data drift after deploying a model?\n",
    "\n",
    "Data drift refers to the phenomenon where the statistical properties of input data change over time compared to the data used during model training. This can manifest as changes in feature distributions, the appearance of new categories, shifts in data ranges, or alterations in the relationships between features. Data drift is crucial to monitor because machine learning models are trained on historical data and assume that future data will follow similar patterns. When the incoming production data diverges significantly from the training data, model performance can degrade substantially, leading to poor predictions and unreliable outputs. For example, a fraud detection model trained before the pandemic might struggle with new fraud patterns that emerged during widespread remote work adoption. By monitoring for data drift, teams can detect when their models need retraining or when input preprocessing needs adjustment, ensuring that models remain accurate and valuable over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b104c4d7-9123-4931-a61e-dc52aa4590fe",
   "metadata": {},
   "source": [
    "##### 3.  Describe the differences between logging and monitoring in DevOps and MLOps. How do they complement each other?\n",
    "\n",
    "While logging and monitoring are closely related concepts, they serve different purposes in both DevOps and MLOps contexts. Logging is primarily about recording discrete events, transactions, and state changes that occur within a system, creating a historical record that can be queried and analyzed after the fact. Monitoring, on the other hand, focuses on observing system behavior in real-time through metrics, dashboards, and alerts, providing immediate visibility into system health and performance. In MLOps specifically, logging might capture individual prediction requests and their features, while monitoring would track aggregate metrics like average prediction latency, model accuracy over time, or resource utilization. These practices complement each other because logs provide the detailed context needed to investigate issues that monitoring alerts identify. When a monitoring dashboard shows that model accuracy has dropped, engineers examine logs to understand which specific inputs or conditions led to poor predictions, creating a complete observability solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafe57c3-62f2-47b5-8f0a-1f8181d74e56",
   "metadata": {},
   "source": [
    "##### 4.   List and briefly describe three common log levels used in logging systems. Provide an example of when each level might be appropriately used.\n",
    "\n",
    "The INFO log level represents normal operational messages that highlight the progress of the application or significant milestones, such as recording when a model successfully loads or when a batch prediction job completes processing. The WARNING level indicates potentially problematic situations that don't prevent the system from functioning but might lead to issues, such as when input data contains unexpected null values that are being handled with default imputation or when API response times are approaching timeout thresholds. The ERROR level signifies serious problems that have caused a particular operation to fail, such as when a model prediction request fails due to invalid input data formats or when the system cannot connect to a required database. Each level serves a different purpose in helping engineers filter through vast amounts of log data to focus on what matters most for their current task, whether that's understanding normal operations, investigating potential issues, or responding to active failures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36ef92c-8131-4a47-8df4-3cfd07db3000",
   "metadata": {},
   "source": [
    "##### 5.  What are the advantages of using a centralized logging system compared to logging to multiple individual sources?\n",
    "\n",
    "Centralized logging systems aggregate logs from multiple sources into a single, searchable repository, offering numerous advantages over distributed logging approaches. When logs are scattered across different servers, containers, or services, troubleshooting becomes tremendously difficult because engineers must access multiple systems to piece together what happened during an incident. A centralized system provides unified search capabilities, allowing teams to correlate events across different components, track a single request as it flows through multiple services, and identify patterns that span the entire infrastructure. Centralized logging also simplifies retention policies, backup strategies, and access control management since there's one system to configure and maintain rather than many. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf3017c-c3e3-4083-84e8-537b40b8584b",
   "metadata": {},
   "source": [
    "##### 6.    In the context of AWS SageMaker, what is the role of the DataCaptureConfig class? Explain its key parameters.\n",
    "The DataCaptureConfig class in AWS SageMaker enables the automatic collection of inference request and response data for deployed models, serving as a foundation for model monitoring and quality assurance. Its key parameters include the capture percentage, which determines what fraction of inference traffic should be logged to avoid overwhelming storage with every single request in high-volume scenarios. The destination S3 URI parameter specifies where captured data should be stored for later analysis. The capture mode parameter allows selection of whether to capture inputs only, outputs only, or both, providing flexibility based on data sensitivity and storage constraints. There's also a KMS encryption key parameter for securing captured data at rest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3bb42e-3299-4fc0-9dc5-eba253276ef9",
   "metadata": {},
   "source": [
    "##### 7.   Compare and contrast the monitoring capabilities of AWS SageMaker and Azure ML. What are some unique features offered by each platform?\n",
    "\n",
    "AWS SageMaker and Azure ML both provide comprehensive monitoring capabilities but with different strengths and approaches. SageMaker offers built-in model monitor capabilities that automatically detect data drift and model quality degradation by comparing production data against baseline datasets, with native integration into CloudWatch for metrics and alerts. It provides specific monitoring types including data quality monitoring, model quality monitoring, bias drift monitoring, and feature attribution drift monitoring. Azure ML takes a more flexible approach with its model data collector and monitoring capabilities, offering tight integration with Azure Monitor and Application Insights for comprehensive observability. A unique Azure feature is its integration with responsible AI dashboards that provide explanability and fairness metrics directly in the monitoring interface. SageMaker's advantage lies in its more automated, turnkey monitoring solutions that require less configuration, while Azure ML offers greater customization and deeper integration with the broader Azure ecosystem, including advanced analytics through Azure Databricks and custom monitoring workflows through Azure Functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec36fa7-0f88-473c-9722-851f7bfdd6ce",
   "metadata": {},
   "source": [
    "##### 8.  Why is it beneficial to integrate logging and monitoring with cloud-native services like Amazon S3 or Azure Blob Storage?\n",
    "\n",
    "Integrating logging and monitoring with cloud-native storage services like Amazon S3 or Azure Blob Storage provides scalability, durability, and cost-effectiveness that would be difficult to achieve with traditional storage solutions. These services offer virtually unlimited storage capacity, allowing systems to retain extensive logs and monitoring data without worrying about running out of disk space. The pay-as-you-go pricing model means organizations only pay for what they actually use, and storage costs decrease significantly for older, less frequently accessed data through automatic tiering to cheaper storage classes. Cloud storage services also provide built-in redundancy and durability guarantees, ensuring that critical log data isn't lost even if individual servers or data centers fail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2278fff-674f-479e-9997-ffcf133925f9",
   "metadata": {},
   "source": [
    "##### 9.  Discuss the potential consequences of not implementing proper logging and monitoring in a production ML system. Provide at least two scenarios.\n",
    "\n",
    "Failing to implement proper logging and monitoring in production ML systems can lead to severe consequences that jeopardize both business outcomes and user trust. In one scenario, a recommendation model might gradually degrade in performance due to undetected data drift, leading to increasingly irrelevant product suggestions that frustrate users and reduce conversion rates over weeks or months before anyone notices the systematic problem. Without monitoring, this silent failure continues causing revenue loss and customer dissatisfaction because there's no automated way to detect that model accuracy has dropped from ninety percent to sixty percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3d5630-6525-48c3-80d7-d6c409c9dfc3",
   "metadata": {},
   "source": [
    "##### 10. Explain the concept of a ’baseline dataset’ in model monitoring. How is it used to detect data drift?\n",
    "\n",
    "A baseline dataset in model monitoring represents the reference data against which production data is compared to detect changes in data characteristics and model behavior. This baseline is typically derived from the training data or from an initial period of production data when the model was known to perform well, establishing expected distributions for each feature, typical ranges for continuous variables, and normal frequencies for categorical variables. Statistical tests are then applied to compare incoming production data against these baseline distributions, calculating metrics like population stability index, Kullback-Leibler divergence, or simple statistical measures like mean and standard deviation differences. When production data significantly diverges from the baseline beyond predefined thresholds, it signals potential data drift that might impact model performance. The baseline essentially answers the question of what normal looks like for your model's inputs and outputs, providing an objective standard for determining when something has changed enough to warrant attention, investigation, or model retraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064d6310-841b-48b3-b893-aca85dcae296",
   "metadata": {},
   "source": [
    "#### Part 2: Hands-On Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a656ff5-68a3-4e8e-89cc-ef73a7a200a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install kaggle pandas numpy scikit-learn boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a2f3ea6-e59a-4e5c-80a2-c404bf389d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d723cdb-2ea5-4f6e-b6d2-0f5949165280",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration\n",
    "BUCKET_NAME = 'your-unique-bucket-name'  # Change this to your unique bucket name\n",
    "DATA_PREFIX = 'data/'\n",
    "DATASET_NAME = 'heart-disease-data'\n",
    "KAGGLE_DATASET = 'redwankarimsony/heart-disease-data'\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "\n",
    "def create_s3_bucket():\n",
    "    \"\"\"Create S3 bucket if it doesn't exist.\"\"\"\n",
    "    try:\n",
    "        # Check if bucket exists\n",
    "        s3_client.head_bucket(Bucket=BUCKET_NAME)\n",
    "        print(f\"Bucket {BUCKET_NAME} already exists\")\n",
    "    except:\n",
    "        try:\n",
    "            # Create bucket\n",
    "            s3_client.create_bucket(Bucket=BUCKET_NAME)\n",
    "            print(f\"Created bucket: {BUCKET_NAME}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bucket: {e}\")\n",
    "            print(\"Please change BUCKET_NAME to a unique name\")\n",
    "            raise\n",
    "\n",
    "\n",
    "def download_kaggle_dataset():\n",
    "    \"\"\"Download dataset from Kaggle.\"\"\"\n",
    "    print(\"Downloading dataset from Kaggle...\")\n",
    "    \n",
    "    # Check if kaggle.json exists\n",
    "    kaggle_config = os.path.expanduser('~/.kaggle/kaggle.json')\n",
    "    if not os.path.exists(kaggle_config):\n",
    "        print(\"\\nKaggle credentials not found!\")\n",
    "        print(\"Please follow these steps:\")\n",
    "        print(\"1. Go to https://www.kaggle.com/settings\")\n",
    "        print(\"2. Click 'Create New API Token'\")\n",
    "        print(\"3. Save kaggle.json to ~/.kaggle/kaggle.json\")\n",
    "        print(\"4. Run: chmod 600 ~/.kaggle/kaggle.json\")\n",
    "        raise FileNotFoundError(\"Kaggle credentials not configured\")\n",
    "    \n",
    "    # Download using kaggle CLI\n",
    "    os.system(f'kaggle datasets download -d {KAGGLE_DATASET} --unzip')\n",
    "    print(\"Dataset downloaded successfully\")\n",
    "\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"Load and preprocess the Heart Disease dataset.\"\"\"\n",
    "    print(\"\\nLoading and preprocessing data...\")\n",
    "    \n",
    "    # Try to find the CSV file\n",
    "    possible_files = [\n",
    "        'heart.csv',\n",
    "        'heart_disease.csv',\n",
    "        'Heart Disease Data.csv',\n",
    "        'data.csv'\n",
    "    ]\n",
    "    \n",
    "    df = None\n",
    "    for file in possible_files:\n",
    "        if os.path.exists(file):\n",
    "            df = pd.read_csv(file)\n",
    "            print(f\"Loaded dataset from: {file}\")\n",
    "            break\n",
    "    \n",
    "    if df is None:\n",
    "        # List all CSV files in current directory\n",
    "        csv_files = [f for f in os.listdir('.') if f.endswith('.csv')]\n",
    "        print(f\"Available CSV files: {csv_files}\")\n",
    "        if csv_files:\n",
    "            df = pd.read_csv(csv_files[0])\n",
    "            print(f\"Loaded dataset from: {csv_files[0]}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(\"No CSV file found. Please check the download.\")\n",
    "    \n",
    "    print(f\"\\nDataset shape: {df.shape}\")\n",
    "    print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "    \n",
    "    # Handle missing values if any\n",
    "    if df.isnull().sum().sum() > 0:\n",
    "        print(\"\\nHandling missing values...\")\n",
    "        # For numerical columns, fill with median\n",
    "        numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numerical_cols:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "        \n",
    "        # For categorical columns, fill with mode\n",
    "        categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "        for col in categorical_cols:\n",
    "            if df[col].isnull().sum() > 0:\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "    \n",
    "    # Encode categorical variables if any\n",
    "    categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        print(f\"\\nEncoding categorical columns: {categorical_cols.tolist()}\")\n",
    "        label_encoders = {}\n",
    "        for col in categorical_cols:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))\n",
    "            label_encoders[col] = le\n",
    "        \n",
    "        # Save label encoders for later use\n",
    "        with open('label_encoders.json', 'w') as f:\n",
    "            # Convert to serializable format\n",
    "            encoders_dict = {col: list(le.classes_) \n",
    "                           for col, le in label_encoders.items()}\n",
    "            json.dump(encoders_dict, f)\n",
    "    \n",
    "    # Identify target column (usually 'target', 'num', or last column)\n",
    "    target_candidates = ['target', 'num', 'disease', 'output', 'label']\n",
    "    target_col = None\n",
    "    \n",
    "    for candidate in target_candidates:\n",
    "        if candidate in df.columns:\n",
    "            target_col = candidate\n",
    "            break\n",
    "    \n",
    "    if target_col is None:\n",
    "        # Assume last column is target\n",
    "        target_col = df.columns[-1]\n",
    "    \n",
    "    print(f\"\\nTarget column identified: {target_col}\")\n",
    "    print(f\"Target distribution:\\n{df[target_col].value_counts()}\")\n",
    "    \n",
    "    # Reorder columns to have target as first column (SageMaker convention)\n",
    "    cols = [target_col] + [col for col in df.columns if col != target_col]\n",
    "    df = df[cols]\n",
    "    \n",
    "    print(f\"\\nFinal preprocessed shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_and_save_data(df):\n",
    "    \"\"\"Split data into train/validation and save as CSV.\"\"\"\n",
    "    print(\"\\nSplitting data into train and validation sets...\")\n",
    "    \n",
    "    # Split 80-20\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, \n",
    "                                        stratify=df.iloc[:, 0])\n",
    "    \n",
    "    print(f\"Training set size: {len(train_df)}\")\n",
    "    print(f\"Validation set size: {len(val_df)}\")\n",
    "    \n",
    "    # Save locally\n",
    "    train_file = 'heart_train.csv'\n",
    "    val_file = 'heart_validation.csv'\n",
    "    \n",
    "    # Save without header and index for SageMaker\n",
    "    train_df.to_csv(train_file, header=False, index=False)\n",
    "    val_df.to_csv(val_file, header=False, index=False)\n",
    "    \n",
    "    # Also save with headers for baseline monitoring\n",
    "    train_df.to_csv('heart_train_with_headers.csv', index=False)\n",
    "    val_df.to_csv('heart_validation_with_headers.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nSaved training data to: {train_file}\")\n",
    "    print(f\"Saved validation data to: {val_file}\")\n",
    "    \n",
    "    return train_file, val_file\n",
    "\n",
    "\n",
    "def upload_to_s3(train_file, val_file):\n",
    "    \"\"\"Upload datasets to S3.\"\"\"\n",
    "    print(f\"\\nUploading datasets to S3 bucket: {BUCKET_NAME}\")\n",
    "    \n",
    "    files_to_upload = [\n",
    "        (train_file, f'{DATA_PREFIX}train/heart_train.csv'),\n",
    "        (val_file, f'{DATA_PREFIX}validation/heart_validation.csv'),\n",
    "        ('heart_train_with_headers.csv', f'{DATA_PREFIX}baseline/heart_train_with_headers.csv'),\n",
    "        ('heart_validation_with_headers.csv', f'{DATA_PREFIX}baseline/heart_validation_with_headers.csv')\n",
    "    ]\n",
    "    \n",
    "    s3_paths = {}\n",
    "    for local_file, s3_key in files_to_upload:\n",
    "        try:\n",
    "            s3_client.upload_file(local_file, BUCKET_NAME, s3_key)\n",
    "            s3_path = f's3://{BUCKET_NAME}/{s3_key}'\n",
    "            s3_paths[local_file] = s3_path\n",
    "            print(f\"Uploaded {local_file} to {s3_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading {local_file}: {e}\")\n",
    "            raise\n",
    "    \n",
    "    return s3_paths\n",
    "\n",
    "\n",
    "def verify_s3_upload():\n",
    "    \"\"\"Verify that files were uploaded successfully.\"\"\"\n",
    "    print(f\"\\nVerifying S3 upload...\")\n",
    "    \n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(\n",
    "            Bucket=BUCKET_NAME,\n",
    "            Prefix=DATA_PREFIX\n",
    "        )\n",
    "        \n",
    "        if 'Contents' in response:\n",
    "            print(f\"\\nFiles in s3://{BUCKET_NAME}/{DATA_PREFIX}:\")\n",
    "            for obj in response['Contents']:\n",
    "                print(f\"  - {obj['Key']} ({obj['Size']} bytes)\")\n",
    "        else:\n",
    "            print(\"No files found in S3 bucket\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error verifying S3 upload: {e}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"Heart Disease Dataset Preparation Pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Step 1: Create S3 bucket\n",
    "    print(\"\\nStep 1: Creating S3 bucket...\")\n",
    "    create_s3_bucket()\n",
    "    \n",
    "    # Step 2: Download dataset from Kaggle\n",
    "    print(\"\\nStep 2: Downloading dataset...\")\n",
    "    try:\n",
    "        download_kaggle_dataset()\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError downloading from Kaggle: {e}\")\n",
    "        print(\"\\nAlternative: Download manually from:\")\n",
    "        print(\"https://www.kaggle.com/datasets/redwankarimsony/heart-disease-data\")\n",
    "        print(\"Extract and place the CSV in the current directory\")\n",
    "        input(\"Press Enter after placing the CSV file...\")\n",
    "    \n",
    "    # Step 3: Load and preprocess\n",
    "    print(\"\\nStep 3: Loading and preprocessing data...\")\n",
    "    df = load_and_preprocess_data()\n",
    "    \n",
    "    # Step 4: Split and save\n",
    "    print(\"\\nStep 4: Splitting and saving data...\")\n",
    "    train_file, val_file = split_and_save_data(df)\n",
    "    \n",
    "    # Step 5: Upload to S3\n",
    "    print(\"\\nStep 5: Uploading to S3...\")\n",
    "    s3_paths = upload_to_s3(train_file, val_file)\n",
    "    \n",
    "    # Step 6: Verify upload\n",
    "    print(\"\\nStep 6: Verifying upload...\")\n",
    "    verify_s3_upload()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Dataset Preparation Complete!\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\nS3 Paths:\")\n",
    "    for file, path in s3_paths.items():\n",
    "        print(f\"  {file}: {path}\")\n",
    "    \n",
    "    print(f\"\\nYou can now proceed to train the model using:\")\n",
    "    print(f\"  Training data: s3://{BUCKET_NAME}/{DATA_PREFIX}train/\")\n",
    "    print(f\"  Validation data: s3://{BUCKET_NAME}/{DATA_PREFIX}validation/\")\n",
    "    print(f\"  Baseline data: s3://{BUCKET_NAME}/{DATA_PREFIX}baseline/\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc7156-2827-4603-b92a-a8b709a503a4",
   "metadata": {},
   "source": [
    "##### Data captured and montoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917a4e3-7990-4708-a490-5f256178c551",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MonitoringVerifier:\n",
    "    \"\"\"Verify monitoring setup and analyze results.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.s3_client = boto3.client('s3')\n",
    "        self.sm_client = boto3.client('sagemaker')\n",
    "        self.cw_client = boto3.client('cloudwatch')\n",
    "        \n",
    "        # Load deployment info\n",
    "        with open('deployment_info.json', 'r') as f:\n",
    "            self.deployment_info = json.load(f)\n",
    "        \n",
    "        self.endpoint_name = self.deployment_info['endpoint_name']\n",
    "        self.data_capture_path = self.deployment_info['data_capture_path']\n",
    "        self.monitoring_schedule_path = self.deployment_info['monitoring_schedule_path']\n",
    "        self.monitoring_schedule_name = self.deployment_info['monitoring_schedule_name']\n",
    "        \n",
    "        # Parse S3 paths\n",
    "        self.bucket = self.data_capture_path.split('/')[2]\n",
    "        self.data_capture_prefix = '/'.join(self.data_capture_path.split('/')[3:])\n",
    "        self.monitoring_prefix = '/'.join(self.monitoring_schedule_path.split('/')[3:])\n",
    "        \n",
    "        print(\"Monitoring Verifier initialized\")\n",
    "        print(f\"Endpoint: {self.endpoint_name}\")\n",
    "        print(f\"Bucket: {self.bucket}\")\n",
    "    \n",
    "    def list_captured_data(self, max_files=20):\n",
    "        \"\"\"List captured data files in S3.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Captured Data Verification\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.bucket,\n",
    "                Prefix=self.data_capture_prefix,\n",
    "                MaxKeys=max_files\n",
    "            )\n",
    "            \n",
    "            if 'Contents' not in response:\n",
    "                print(f\"No captured data found in s3://{self.bucket}/{self.data_capture_prefix}\")\n",
    "                print(\"\\nPossible reasons:\")\n",
    "                print(\"1. No prediction requests have been made yet\")\n",
    "                print(\"2. Data capture may take a few minutes to appear\")\n",
    "                print(\"3. Run generate_traffic.py to create prediction traffic\")\n",
    "                return []\n",
    "            \n",
    "            files = response['Contents']\n",
    "            print(f\"\\nFound {len(files)} captured data files:\")\n",
    "            \n",
    "            total_size = 0\n",
    "            for i, obj in enumerate(files[:max_files], 1):\n",
    "                size_kb = obj['Size'] / 1024\n",
    "                total_size += obj['Size']\n",
    "                print(f\"{i}. {obj['Key']}\")\n",
    "                print(f\"   Size: {size_kb:.2f} KB\")\n",
    "                print(f\"   Modified: {obj['LastModified']}\")\n",
    "            \n",
    "            print(f\"\\nTotal captured data size: {total_size / 1024 / 1024:.2f} MB\")\n",
    "            \n",
    "            return files\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error listing captured data: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def inspect_captured_file(self, file_key=None):\n",
    "        \"\"\"Inspect a captured data file.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Captured Data Inspection\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Get first file if none specified\n",
    "            if file_key is None:\n",
    "                response = self.s3_client.list_objects_v2(\n",
    "                    Bucket=self.bucket,\n",
    "                    Prefix=self.data_capture_prefix,\n",
    "                    MaxKeys=1\n",
    "                )\n",
    "                \n",
    "                if 'Contents' not in response:\n",
    "                    print(\"No captured files to inspect\")\n",
    "                    return\n",
    "                \n",
    "                file_key = response['Contents'][0]['Key']\n",
    "            \n",
    "            print(f\"\\nInspecting: {file_key}\")\n",
    "            \n",
    "            # Download and read file\n",
    "            response = self.s3_client.get_object(Bucket=self.bucket, Key=file_key)\n",
    "            content = response['Body'].read().decode('utf-8')\n",
    "            \n",
    "            # Parse JSONL format\n",
    "            lines = content.strip().split('\\n')\n",
    "            print(f\"Number of records: {len(lines)}\")\n",
    "            \n",
    "            # Display first few records\n",
    "            print(\"\\nFirst 3 records:\")\n",
    "            for i, line in enumerate(lines[:3], 1):\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    print(f\"\\nRecord {i}:\")\n",
    "                    print(f\"  Capture time: {record.get('eventMetadata', {}).get('inferenceTime', 'N/A')}\")\n",
    "                    \n",
    "                    # Extract input\n",
    "                    if 'captureData' in record:\n",
    "                        input_data = record['captureData'].get('endpointInput', {})\n",
    "                        output_data = record['captureData'].get('endpointOutput', {})\n",
    "                        \n",
    "                        print(f\"  Input mode: {input_data.get('mode', 'N/A')}\")\n",
    "                        print(f\"  Output mode: {output_data.get('mode', 'N/A')}\")\n",
    "                        \n",
    "                        # Show data preview\n",
    "                        if 'data' in input_data:\n",
    "                            data_preview = input_data['data'][:100]\n",
    "                            print(f\"  Input preview: {data_preview}...\")\n",
    "                        \n",
    "                        if 'data' in output_data:\n",
    "                            print(f\"  Output: {output_data['data']}\")\n",
    "                    \n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"  Error parsing record: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error inspecting captured file: {e}\")\n",
    "    \n",
    "    def list_monitoring_reports(self):\n",
    "        \"\"\"List monitoring execution reports.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Monitoring Reports Verification\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            response = self.s3_client.list_objects_v2(\n",
    "                Bucket=self.bucket,\n",
    "                Prefix=self.monitoring_prefix\n",
    "            )\n",
    "            \n",
    "            if 'Contents' not in response:\n",
    "                print(f\"No monitoring reports found in s3://{self.bucket}/{self.monitoring_prefix}\")\n",
    "                print(\"\\nNote: Monitoring runs on schedule (daily)\")\n",
    "                print(\"Reports appear after first scheduled execution\")\n",
    "                return []\n",
    "            \n",
    "            files = response['Contents']\n",
    "            print(f\"\\nFound {len(files)} monitoring report files:\")\n",
    "            \n",
    "            # Group by execution\n",
    "            executions = {}\n",
    "            for obj in files:\n",
    "                parts = obj['Key'].split('/')\n",
    "                if len(parts) >= 3:\n",
    "                    exec_id = parts[-2]\n",
    "                    if exec_id not in executions:\n",
    "                        executions[exec_id] = []\n",
    "                    executions[exec_id].append(obj)\n",
    "            \n",
    "            for exec_id, exec_files in executions.items():\n",
    "                print(f\"\\nExecution: {exec_id}\")\n",
    "                for obj in exec_files:\n",
    "                    filename = obj['Key'].split('/')[-1]\n",
    "                    print(f\"  - {filename} ({obj['Size']} bytes)\")\n",
    "            \n",
    "            return list(executions.keys())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error listing monitoring reports: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def analyze_violations(self, execution_id=None):\n",
    "        \"\"\"Analyze constraint violations from monitoring reports.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Violations Analysis\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            # Get latest execution if none specified\n",
    "            if execution_id is None:\n",
    "                response = self.s3_client.list_objects_v2(\n",
    "                    Bucket=self.bucket,\n",
    "                    Prefix=self.monitoring_prefix\n",
    "                )\n",
    "                \n",
    "                if 'Contents' not in response:\n",
    "                    print(\"No monitoring reports available yet\")\n",
    "                    return\n",
    "                \n",
    "                # Find constraint_violations.json files\n",
    "                violation_files = [\n",
    "                    obj for obj in response['Contents']\n",
    "                    if 'constraint_violations.json' in obj['Key']\n",
    "                ]\n",
    "                \n",
    "                if not violation_files:\n",
    "                    print(\"No violation files found - monitoring may not have run yet\")\n",
    "                    return\n",
    "                \n",
    "                # Use most recent\n",
    "                violation_files.sort(key=lambda x: x['LastModified'], reverse=True)\n",
    "                violation_key = violation_files[0]['Key']\n",
    "            else:\n",
    "                violation_key = f\"{self.monitoring_prefix}/{execution_id}/constraint_violations.json\"\n",
    "            \n",
    "            print(f\"\\nAnalyzing violations from: {violation_key}\")\n",
    "            \n",
    "            # Download violations file\n",
    "            response = self.s3_client.get_object(Bucket=self.bucket, Key=violation_key)\n",
    "            violations = json.loads(response['Body'].read().decode('utf-8'))\n",
    "            \n",
    "            if 'violations' not in violations or len(violations['violations']) == 0:\n",
    "                print(\"\\nNo violations detected - data distribution is within baseline constraints\")\n",
    "                return\n",
    "            \n",
    "            print(f\"\\nTotal violations: {len(violations['violations'])}\")\n",
    "            \n",
    "            # Analyze violations by type\n",
    "            violation_types = {}\n",
    "            for violation in violations['violations']:\n",
    "                v_type = violation.get('constraint_check_type', 'Unknown')\n",
    "                if v_type not in violation_types:\n",
    "                    violation_types[v_type] = []\n",
    "                violation_types[v_type].append(violation)\n",
    "            \n",
    "            print(\"\\nViolations by type:\")\n",
    "            for v_type, v_list in violation_types.items():\n",
    "                print(f\"\\n{v_type}: {len(v_list)} violations\")\n",
    "                \n",
    "                # Show first few violations\n",
    "                for v in v_list[:3]:\n",
    "                    print(f\"  Feature: {v.get('feature_name', 'Unknown')}\")\n",
    "                    print(f\"  Description: {v.get('description', 'N/A')}\")\n",
    "            \n",
    "            # Create summary DataFrame\n",
    "            summary_data = []\n",
    "            for violation in violations['violations']:\n",
    "                summary_data.append({\n",
    "                    'Feature': violation.get('feature_name', 'Unknown'),\n",
    "                    'Type': violation.get('constraint_check_type', 'Unknown'),\n",
    "                    'Description': violation.get('description', 'N/A')[:60]\n",
    "                })\n",
    "            \n",
    "            if summary_data:\n",
    "                summary_df = pd.DataFrame(summary_data)\n",
    "                print(\"\\nViolations Summary:\")\n",
    "                print(summary_df.to_string(index=False))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing violations: {e}\")\n",
    "    \n",
    "    def check_monitoring_schedule_status(self):\n",
    "        \"\"\"Check monitoring schedule status.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Monitoring Schedule Status\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            response = self.sm_client.describe_monitoring_schedule(\n",
    "                MonitoringScheduleName=self.monitoring_schedule_name\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nSchedule Name: {response['MonitoringScheduleName']}\")\n",
    "            print(f\"Status: {response['MonitoringScheduleStatus']}\")\n",
    "            print(f\"Schedule: {response['MonitoringScheduleConfig']['ScheduleConfig']['ScheduleExpression']}\")\n",
    "            \n",
    "            if 'LastMonitoringExecutionSummary' in response:\n",
    "                last_exec = response['LastMonitoringExecutionSummary']\n",
    "                print(f\"\\nLast Execution:\")\n",
    "                print(f\"  Status: {last_exec['MonitoringExecutionStatus']}\")\n",
    "                print(f\"  Scheduled Time: {last_exec['ScheduledTime']}\")\n",
    "                print(f\"  Created Time: {last_exec['CreatedTime']}\")\n",
    "                \n",
    "                if 'EndTime' in last_exec:\n",
    "                    print(f\"  End Time: {last_exec['EndTime']}\")\n",
    "                    duration = (last_exec['EndTime'] - last_exec['CreatedTime']).total_seconds()\n",
    "                    print(f\"  Duration: {duration:.0f} seconds\")\n",
    "                \n",
    "                if 'FailureReason' in last_exec:\n",
    "                    print(f\"  Failure Reason: {last_exec['FailureReason']}\")\n",
    "            else:\n",
    "                print(\"\\nNo executions yet\")\n",
    "                print(\"Monitoring will run at next scheduled time\")\n",
    "            \n",
    "            # List all executions\n",
    "            executions = self.sm_client.list_monitoring_executions(\n",
    "                MonitoringScheduleName=self.monitoring_schedule_name,\n",
    "                MaxResults=5\n",
    "            )\n",
    "            \n",
    "            if 'MonitoringExecutionSummaries' in executions:\n",
    "                print(f\"\\nRecent Executions ({len(executions['MonitoringExecutionSummaries'])}):\")\n",
    "                for exec_summary in executions['MonitoringExecutionSummaries']:\n",
    "                    print(f\"  - {exec_summary['MonitoringExecutionStatus']} at {exec_summary['ScheduledTime']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking monitoring schedule: {e}\")\n",
    "    \n",
    "    def check_cloudwatch_metrics(self, hours=24):\n",
    "        \"\"\"Check CloudWatch metrics for the endpoint.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CloudWatch Metrics\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        try:\n",
    "            end_time = datetime.utcnow()\n",
    "            start_time = end_time - timedelta(hours=hours)\n",
    "            \n",
    "            # Define metrics to check\n",
    "            metrics = [\n",
    "                ('ModelInvocations', 'Sum'),\n",
    "                ('ModelLatency', 'Average'),\n",
    "                ('feature_baseline_drift', 'Average')\n",
    "            ]\n",
    "            \n",
    "            print(f\"\\nQuerying metrics from {start_time} to {end_time}\")\n",
    "            \n",
    "            for metric_name, stat in metrics:\n",
    "                print(f\"\\n{metric_name}:\")\n",
    "                \n",
    "                try:\n",
    "                    response = self.cw_client.get_metric_statistics(\n",
    "                        Namespace='AWS/SageMaker',\n",
    "                        MetricName=metric_name,\n",
    "                        Dimensions=[\n",
    "                            {'Name': 'EndpointName', 'Value': self.endpoint_name}\n",
    "                        ],\n",
    "                        StartTime=start_time,\n",
    "                        EndTime=end_time,\n",
    "                        Period=3600,  # 1 hour\n",
    "                        Statistics=[stat]\n",
    "                    )\n",
    "                    \n",
    "                    if not response['Datapoints']:\n",
    "                        print(\"  No data available\")\n",
    "                        continue\n",
    "                    \n",
    "                    datapoints = sorted(response['Datapoints'], key=lambda x: x['Timestamp'])\n",
    "                    \n",
    "                    for dp in datapoints[-5:]:  # Show last 5\n",
    "                        value = dp[stat]\n",
    "                        timestamp = dp['Timestamp']\n",
    "                        print(f\"  {timestamp}: {value:.2f}\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  Error: {e}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error checking CloudWatch metrics: {e}\")\n",
    "    \n",
    "    def generate_report(self):\n",
    "        \"\"\"Generate comprehensive verification report.\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COMPREHENSIVE MONITORING VERIFICATION REPORT\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        report = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'endpoint': self.endpoint_name,\n",
    "            'bucket': self.bucket\n",
    "        }\n",
    "        \n",
    "        # Check data capture\n",
    "        print(\"\\n1. Checking data capture...\")\n",
    "        captured_files = self.list_captured_data(max_files=10)\n",
    "        report['captured_files_count'] = len(captured_files)\n",
    "        \n",
    "        if captured_files:\n",
    "            self.inspect_captured_file()\n",
    "        \n",
    "        # Check monitoring reports\n",
    "        print(\"\\n2. Checking monitoring reports...\")\n",
    "        executions = self.list_monitoring_reports()\n",
    "        report['monitoring_executions'] = len(executions)\n",
    "        \n",
    "        if executions:\n",
    "            self.analyze_violations()\n",
    "        \n",
    "        # Check schedule status\n",
    "        print(\"\\n3. Checking monitoring schedule...\")\n",
    "        self.check_monitoring_schedule_status()\n",
    "        \n",
    "        # Check CloudWatch\n",
    "        print(\"\\n4. Checking CloudWatch metrics...\")\n",
    "        self.check_cloudwatch_metrics()\n",
    "        \n",
    "        # Save report\n",
    "        with open('verification_report.json', 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Verification Complete\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Report saved to: verification_report.json\")\n",
    "        \n",
    "        return report\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function.\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"SageMaker Model Monitoring Verification\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    verifier = MonitoringVerifier()\n",
    "    verifier.generate_report()\n",
    "    \n",
    "    print(\"\\nVerification complete!\")\n",
    "    print(\"\\nNext steps:\")\n",
    "    print(\"1. If no captured data: Run generate_traffic.py\")\n",
    "    print(\"2. If no monitoring reports: Wait for scheduled execution\")\n",
    "    print(\"3. Review CloudWatch dashboard for real-time metrics\")\n",
    "    print(\"4. Check S3 bucket for all artifacts\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6512fe-c2da-4bb6-be7e-1fb21627e33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
